{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1.102\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import mdml_client as mdml\n",
    "print(mdml.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MDML Producing Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_data = {\n",
    "    'time': time.time(), \n",
    "    'int1': 1,\n",
    "    'int2': 2,\n",
    "    'int3': 3\n",
    "}\n",
    "schema = mdml.create_schema(example_data, \"Example schema\", \"schema for example notebook\")\n",
    "producer = mdml.kafka_mdml_producer(\n",
    "    topic = \"mdml-example-dict\",\n",
    "    schema = schema,\n",
    "    kafka_host = '100.26.16.4',\n",
    "    schema_host = '100.26.16.4'\n",
    ")\n",
    "producer.produce(example_data)\n",
    "producer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MDML Consuming Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consumer loop will exit after 300.0 seconds without receiving a message or with Ctrl+C\n",
      "{'topic': 'mdml-example-dict', 'value': {'time': 1631221786.1839342, 'int1': 1, 'int2': 2, 'int3': 3}}\n"
     ]
    }
   ],
   "source": [
    "consumer = mdml.kafka_mdml_consumer(\n",
    "    topics = [\"mdml-example-dict\"],\n",
    "    group = \"abc\", # create a unique group id here\n",
    "    kafka_host = '100.26.16.4',\n",
    "    schema_host = '100.26.16.4'\n",
    ")\n",
    "for msg in consumer.consume():\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming Files via MDML\n",
    "\n",
    "The MDML takes two approaches to streaming large files. One is by chunking and the other we call \"coat-checking\". In chunking, a large file is broken up into smaller chunks that are sent directly to the MDML. We will only demonstrate the chunking method here. The second method of \"coat-checking\" uses an S3 bucket to upload files. At the same time, a message describing the location and some metadata about the file is sent to the MDML. A consumer could then download the file from the specified S3 bucket location in the message.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "flush\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "flush\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "flush\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "flush\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "flush\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "final flush\n"
     ]
    }
   ],
   "source": [
    "large_file = \"large_file.txt\" # ~20MB\n",
    "producer = mdml.kafka_mdml_producer(\n",
    "    topic = \"mdml-example-file\",\n",
    "    schema = mdml.multipart_schema, # using MDML's pre-defined schema for chunking\n",
    "    kafka_host = '100.26.16.4',\n",
    "    schema_host = '100.26.16.4'\n",
    ")\n",
    "i=0\n",
    "for chunk in mdml.chunk_file(large_file, 500000): # chunk size of 500,000 Bytes\n",
    "    producer.produce(chunk)\n",
    "    i += 1\n",
    "    if i % 10 == 0:\n",
    "        print(\"flush\")\n",
    "        producer.flush() # flush every 50 chunks\n",
    "print(\"final flush\")\n",
    "producer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consumer loop will exit after 300.0 seconds without receiving a message or with Ctrl+C\n",
      "(1631221960.0640821, 'large_file.txt')\n"
     ]
    }
   ],
   "source": [
    "consumer = mdml.kafka_mdml_consumer(\n",
    "    topics = [\"mdml-example-file\"],\n",
    "    group = \"abc\", # create a unique group id here\n",
    "    kafka_host = '100.26.16.4',\n",
    "    schema_host = '100.26.16.4'\n",
    ")\n",
    "for msg in consumer.consume_chunks(): # the message returned is the filepath that the chunked file was written to\n",
    "    print(msg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mdml",
   "language": "python",
   "name": "mdml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
